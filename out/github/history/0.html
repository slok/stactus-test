
<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Github status</title>

    <link rel="stylesheet" href="https://slok.github.io/stactus-test/github/static/main.css" />
</head>


<body>
    <div class="wrapper">
        <h1>Incident History</h1>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/9dyp7zt86rp5">Disruption with some GitHub services</a></h2>
            <p>
                <p>Between August 27, 2024, 15:52 UTC and September 5, 2024, 17:26 UTC the GitHub Connect service was degraded. This specifically impacted GHES customers who were enabling GitHub Connect for the first time on a GHES instance. Previously enabled GitHub Connect GHES instances were not impacted by this issue.<br /><br />Customers experiencing this issue would have received a 404 response during GitHub Connect enablement and subsequent messages about a failure to connect. This was due to a recent change in configuration to GitHub Connect which has since been rolled back. <br /><br />Subsequent enablement failures on re-attempts were caused by data corruption which has been remediated. Customers should now be able to enable GitHub Connect successfully.<br /><br />To reduce our time to detection and mitigation of such issues in the future, we are working to improve observability of GitHub Connect failures. We are also making efforts to prevent future misconfiguration of GitHub Connect.</p>

            </p>

            <br>
            <small>
            
                Sep  5, 15:55 - Sep  5, 17:24
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/djwfzglh2dyq">Disruption with some GitHub services</a></h2>
            <p>
                <p>On August 29th, 2024, from 16:56 UTC to 21:42 UTC, we observed an elevated rate of traffic on our public edge, which triggered GitHubâ€™s rate limiting protections. This resulted in <0.1% of users being identified as false-positives, which they experienced as intermittent connection timeouts. At 20:59 UTC the engineering team improved the system to remediate the false-positive identification of user traffic, and return to normal traffic operations.</p>

            </p>

            <br>
            <small>
            
                Aug 29, 19:29 - Aug 29, 21:54
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/345t4sjty6q3">Disruption with some GitHub services</a></h2>
            <p>
                <p>On August 28, 2024, from 21:40 to 23:43 UTC, up to 25% of unauthenticated dotcom traffic in SE Asia (representing % of global traffic) encountered HTTP 500 errors. We observed elevated error rates at one of our global points of presence, where geo-DNS health checks were failing. We identified unhealthy cloud hardware in the region, indicated by abnormal CPU utilization patterns. As a result, we drained the site at 23:26 UTC, which promptly restored normal traffic operations.</p>

            </p>

            <br>
            <small>
            
                Aug 28, 22:02 - Aug 28, 23:43
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/1zyrr8fxrl21">Disruption with some GitHub services</a></h2>
            <p>
                <p>This incident has been resolved.</p>

            </p>

            <br>
            <small>
            
                Aug 27, 22:37 - Aug 27, 23:26
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/30jg3tzwbv93">Incident with Actions</a></h2>
            <p>
                <p>On August 22, 2024, between 16:10 UTC and 17:28 UTC, Actions experienced degraded performance leading to failed workflow runs. On average, 2.5% of workflow runs failed to start with the failure rate peaking at 6%. In addition we saw a 1% error rate for Actions API endpoints. This was due to an Actions service being deployed to faulty hardware that had an incorrect memory configuration, leading to significant performance degradation of those pods due to insufficient memory.<br /><br />The impact was mitigated when the pods were evicted automatically and moved to healthy hosts. The faulty hardware was disabled to prevent a recurrence. We are improving our health checks to ensure that unhealthy hardware is consistently marked offline automatically. We are also improving our monitoring and deployment practices to reduce our time to detection and automated mitigation at the service layer for issues like this in the future.</p>

            </p>

            <br>
            <small>
            
                Aug 22, 16:49 - Aug 22, 17:28
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/3vhw289ftnqp">Incident with Actions</a></h2>
            <p>
                <p>On August 21, 2024, between 13:48 UTC and 15:00 UTC, Actions experienced degraded performance, leading to delays in workflow runs. On average, 25% of workflow runs were delayed by 8 minutes. Less than 1% of workflow runs exhausted retries and failed to start. The issue stemmed from a backlog of Pull Request events which caused delays in Actions processing the event queues that trigger workflow runs.<br /><br />We mitigated the incident by disabling the process that led to the sudden spike in Pull Request events. We are working to improve our monitoring and deployment practices to reduce our time to detection and mitigation of issues like this one in the future. We are also identifying appropriate changes to rate limits and reserved capacity to reduce the breadth of impact.</p>

            </p>

            <br>
            <small>
            
                Aug 21, 14:09 - Aug 21, 15:11
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/mrpx4trfk45z">Incident with starting Action Workflows</a></h2>
            <p>
                <p>On August 15, 2024, between 13:14 UTC and 13:43 UTC, the Actions service was degraded and resulted in failures to start new workflow runs for customers of github.com. On average, 10% of Actions workflow runs failed to start with the failure rate peaking at 15%. This was due to an infrastructure change that enabled a network proxy for requests between the Actions service and an internal API which caused requests to fail.<br /><br />We mitigated the incident by rolling back the change. We are working to improve our monitoring and deployment practices to reduce our time to detection and mitigation of issues like this one in the future.</p>

            </p>

            <br>
            <small>
            
                Aug 15, 13:35 - Aug 15, 13:59
            
            </small>
        </div>
        
        <div class="box impact-major">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/kz4khcgdsfdv">All GitHub services are experiencing significant disruptions</a></h2>
            <p>
                <p>On August 14, 2024 between 23:02 UTC and 23:38 UTC, all GitHub services on GitHub.com were inaccessible for all users.<br /> <br />This was due to a configuration change that impacted traffic routing within our database infrastructure, resulting in critical services unexpectedly losing database connectivity. There was no data loss or corruption during this incident.<br /> <br />At 22:59 UTC an erroneous configuration change rolled out to all GitHub.com databases that impacted the ability of the database to respond to health check pings from the routing service. As a result, the routing service could not detect healthy databases to route application traffic to. This led to widespread impact on GitHub.com starting at 23:02 UTC.<br /> <br />We mitigated the incident by reverting the change and confirming restored connectivity to our databases. At 23:38 UTC, traffic resumed and all services recovered to full health. Out of an abundance of caution, we continued to monitor before resolving the incident at 00:30 UTC on August 15th, 2024.<br /> <br />To prevent recurrence we are implementing additional guardrails in our database change management process. We are also prioritizing several repair items such as faster rollback functionality and more resilience to dependency failures.<br /> <br />Given the severity of this incident, follow-up items are the highest priority work for teams at this time.</p>

            </p>

            <br>
            <small>
            
                Aug 14, 23:11 - Aug 15, 00:30
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/2qxyr481ywxf">Disruption with some GitHub services</a></h2>
            <p>
                <p>On August 13, 2024, between 13:00 UTC and 13:23 UTC the Copilot service and some parts of the GitHub UI were degraded. This impacted about 25% of GitHub.com users. This was due to a partial rollout of a caching layer for Copilot licensing checks. During the rollout, connections to the caching layer were overwhelmed causing the licensing checks to timeout. Many pages were impacted by this failure due to a lack of resiliency to the timeouts.<br /><br />We mitigated the incident by reverting the rollout of the caching layer 11 minutes after initial detection. This immediately restored functionality for affected users.<br /><br />We are working to gracefully degrade experiences during these types of failures and reduce dependencies across services that may cause these types of failures in the future.<br /></p>

            </p>

            <br>
            <small>
            
                Aug 13, 13:11 - Aug 13, 13:23
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="https://slok.github.io/stactus-test/github/ir/ndlzqycv0dh8">Incident with Webhooks</a></h2>
            <p>
                <p>On August 12, 2024 from 13:39 to 14:28 UTC some users experienced an elevated rate of errors of up to 0.45% from the GitHub API. Less than 5% of webhooks interactions failed and less than 0.5% of Actions runs were delayed.<br /><br />This impact was caused by internal networking instances being insufficiently scaled.<br /><br />We mitigated the incident by provisioning additional instances. We are working to enhance the sizing strategy for the relevant infrastructure to prevent similar issues and to also improve monitoring and processes to reduce our time to detection and mitigation of issues like this one in the future.</p>

            </p>

            <br>
            <small>
            
                Aug 12, 14:09 - Aug 12, 14:41
            
            </small>
        </div>
        
        <div class="row">
            
                <div class="block">
                    <a href="https://slok.github.io/stactus-test/github/history/1"> â®œ Previous </a>
                </div>
            
            
        </div>
    </div>
    
<footer></footer>

</body>

</html>
