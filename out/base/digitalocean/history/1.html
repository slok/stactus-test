
<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Digital Ocean status</title>

    <link rel="stylesheet" href="http://127.0.0.1:8080/base/digitalocean/css/main.css" />
</head>


<body>
    <div class="wrapper">
        <h1>Incident History</h1>
        
        <div class="box impact-none">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/1lhs6n3vrjcz">Container Registry and App Platform in AMS3/FRA1</a></h2>
            <p>
                <p>From 10:30 to 11:35 UTC, our Engineering team observed an issue with Container Registry and App Platform builds in the AMS3 and FRA1 regions.</p>

<p>During this time, users may have experienced delays while building their Apps and could have potentially experienced timeout errors in builds as a result. Additionally, a subset of customers may have experienced latency while interacting with the Container Registries.</p>

<p>Our Engineering team found that a backing component of the Container Registry was experiencing high memory usage. They were able to remediate that component at 11:35 UTC, which resolved the issue.</p>

<p>We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.</p>

            </p>

            <br>
            <small>
            
                Aug 22, 15:04 - Aug 22, 15:04
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/bcbmw7dyzlst">Networking in SFO2 and SFO3 region</a></h2>
            <p>
                <p>As of 10:45 UTC, our engineering team has resolved the issue with networking in SFO2 and SFO3 regions, and networking in the regions should now be operating normally. If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.</p>

            </p>

            <br>
            <small>
            
                Aug 15, 10:02 - Aug 15, 11:48
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/24p130g20tff">Event processing and Droplet Creates in TOR1</a></h2>
            <p>
                <p>Our Engineering team has resolved the issue with Droplet creates and Snapshots. As of 05:30 UTC, users should be able to create Droplets, Snapshots and process events. Droplet backed services should also be operating normally.</p>

<p>If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.</p>

            </p>

            <br>
            <small>
            
                Aug 15, 04:29 - Aug 15, 05:40
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/dl057g40dnyx">Elevated Failure Rate for Droplet Creates</a></h2>
            <p>
                <p>From 23:08 August 12 to 01:26 August 13 UTC, customers may have experienced failures with Droplet creation, power on events, and restore events in the NYC3 region.</p>

<p>Our Engineering team has confirmed resolution of this issue. Thank you for your patience.</p>

<p>If you continue to experience any problems, please open a support ticket from within your account.</p>

            </p>

            <br>
            <small>
            
                Aug 13, 00:27 - Aug 13, 03:10
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/24714nw9b7jt">Droplet Rebuild and Restore Event Processing</a></h2>
            <p>
                <p>From 09:52 UTC to 19:32 UTC, customers may have experienced failures with Droplet rebuild and restore events in all regions. Our Engineering team has confirmed full resolution of this issue.</p>

<p>Thank you for your patience. If you continue to experience any problems, please open a support ticket from within your account.</p>

            </p>

            <br>
            <small>
            
                Aug  6, 19:19 - Aug  6, 20:40
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/222822dwq709">Global Impact to Events, Cloud Control Panel, and API</a></h2>
            <p>
                <h3 id="incident-summary"><strong>Incident Summary</strong></h3>

<p>On August 05, 2024 at 16:30 UTC, DigitalOcean experienced a disruption to internal service discovery. Customers experienced full disruption of creates, event processing, and management of other DigitalOcean products globally. Due to an error in a replication configuration that propagated globally, internal services were unable to correctly discover other services they depended on. This did not affect the availability of existing customer resources.</p>

<h3 id="incident-details"><strong>Incident Details</strong></h3>

<p><strong>Root Cause</strong>: An incorrect replication configuration was deployed against the datastore which powers the internal service discovery service at DigitalOcean. The incorrect configuration specified a new datacenter with zero keys as 100% ownership of all keys in the datastore. This had an immediate global impact against the data storage layer and disrupted the quorum of datastore nodes across all regions. Clients of the service were unable to read/write to the datastore during this time, which had a cascading effect.‌</p>

<p><strong>Impact</strong>:</p>

<p>The first observable impact was a complete disruption to the I/O layer of the backing datastore.</p>

<p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdU0ECDjKRvdaKFCHqMyxYzFgFPwuYW_vbTLkcqmh4-YHquKP77AjozHffhv0xPnUNrYXtRV0rPwUYKEtb4n0Dozo_bvHFHbIY986AIirTvtuyDRh_7Z0TYjnUEjd9p1NWB83mr9SDE2Lhz6Fh6eRB_O-LY?key=ADgogSsNR7W3UpxAWFv39Q" alt="" />
<img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeXhrWQUmlgO4kI7yfvDmzeVQNT0oCdDLBi53_P2TYzm8Zk0zZ0btTlKOz4zCPUvMH13byX4GTGUODvnCICI_UQGGNK5uN_YRSHE9RZ0YbQVCV4MZSKBRA1C-heSuzPN800iamOSr8XG0EYCnfK-6Y79mKE?key=ADgogSsNR7W3UpxAWFv39Q" alt="" /></p>

<p>These events are consumed by a wide variety of backing services that compose the DigitalOcean Cloud platform. This incident impacted:</p>

<ul>
<li>Droplet Creates</li>
<li>Droplet Updates</li>
<li>Network Creates</li>
<li>Login / Authentication services</li>
<li>Block Storage Volumes Snapshot creation</li>
<li>Spaces/CDN Creates</li>
<li>Spaces Updates</li>
<li>Managed Kubernetes cluster creates</li>
<li>Managed Databases creates</li>
</ul>

<p>Other services across DigitalOcean, outside of the eventing flow, also rely on service discovery to talk to each other, so customers may have seen additional impact when attempting to manage assorted services through the Cloud Control Panel or via the API.</p>

<p><strong>Response</strong>: After gathering diagnostic information and determining the root cause, an updated / correct replication configuration was deployed. Some regions ingested the new replication configuration and started to recover. Teams identified additional regions that took longer to ingest the updated configuration and manually invoked the change directly on the nodes, and then ran local repairs on the data to ensure alignment before moving to the next region.</p>

<p>Engineering teams cleaned up any remaining failed events and processed pending events that had not yet timed out. At the conclusion of that cleanup effort, the incident was declared resolved, and the cloud platform stabilized.</p>

<h3 id="timeline-of-events-utc"><strong>Timeline of Events (UTC)</strong></h3>

<p>Aug 05 16:30 - Rollout of the new datastore cluster begins.</p>

<p>Aug 05 16:35 - First report of service discovery unavailability is raised internally.</p>

<p>Aug 05 16:42 - Lack of quorum and datastore ownership is identified as the blocking issue.</p>

<p>Aug 05 17:00 - The replication configuration change, adding the new datacenter, is identified as the root cause behind the ownership change.</p>

<p>Aug 05 17:16 - The replication configuration change is reverted, and run against the region that had become the datastore owner. Some events start to fail faster at this point, changing the error from a distinct timeout to a failure to find endpoints.</p>

<p>Aug 05 18:25 - Regions that have not detected or applied the reverted configuration are identified, and engineers start manually applying the configuration and running repairs on the datastore for those regions.</p>

<p>Aug 05 19:10 - Remaining failure events resolve, and the platform stabilizes.</p>

<h3 id="remediation-actions"><strong>Remediation Actions</strong></h3>

<p>The replication configuration deployment happened outside of a normal maintenance window. Moving forward, these types of extension maintenances will be performed inside a declared maintenance window, with any potential for customer impact communicated via a maintenance notice posted on the status page.</p>

<p>The process documentation for this type of deployment will be updated to reflect the current requirements and clearly outline the steps and expectations for each stage of a new deployment. Additionally, the manual processes that occurred will be automated to help reduce the potential for human error.</p>

<p>Multiple teams are also evaluating if our current topology of the internal datastore is appropriate, and if there are any regionalizations or multi-layered approaches DigitalOcean can take to help ensure our internal service discovery remains as available as possible.</p>

            </p>

            <br>
            <small>
            
                Aug  5, 17:05 - Ongoing
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/pdvg7yvldz8s">App Platform and Container Registry in NYC</a></h2>
            <p>
                <p>Our Engineering team has confirmed the full resolution of the issue with the DigitalOcean App Platform and Container Registry in our NYC regions.</p>

<p>Users should no longer experience any issues while pushing to Container Registries and working with App Platform builds.</p>

<p>If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.</p>

            </p>

            <br>
            <small>
            
                Aug  5, 03:21 - Aug  5, 05:48
            
            </small>
        </div>
        
        <div class="box impact-none">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/h0gnwb5h2zty">Spaces Access Key</a></h2>
            <p>
                <p>From 23:47 UTC until 01:11 UTC, users may have experienced errors when attempting to create Spaces Access Keys in the Cloud Control Panel.</p>

<p>Our Engineering team has identified and resolved the issue. The impact has been resolved and users should now be able to create Spaces Access Keys.</p>

<p>We apologize for any inconvenience this may have caused. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.</p>

            </p>

            <br>
            <small>
            
                Jul 31, 01:29 - Jul 31, 01:29
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/p2f2h8vbc23p">Snapshots and Backups in TOR1</a></h2>
            <p>
                <p>As of 05:05 UTC, our Engineering team has confirmed the full resolution of the issue impacting Snapshot and Backup Images in the TOR1 region. We have verified that the Snapshot and Backup events in the region are processing without any failures.</p>

<p>Users should also be able to create Droplets from Snapshot and Backup images in this region without any issues.</p>

<p>Thank you for your patience and understanding. If you should encounter any further issues at all, then please open a ticket with our Support team.</p>

            </p>

            <br>
            <small>
            
                Jul 29, 04:27 - Jul 29, 05:52
            
            </small>
        </div>
        
        <div class="box impact-minor">
            <h2><a href="http://127.0.0.1:8080/base/digitalocean/ir/lxghl0xl5c52">Networking in Multiple Regions</a></h2>
            <p>
                <h3 id="incident-summary"><strong>Incident Summary</strong></h3>

<p>On July 24, 2024, DigitalOcean experienced downtime from near-simultaneous crashes affecting multiple hypervisors (ref: <a href="https://docs.digitalocean.com/glossary/hypervisor/" target="_blank">https://docs.digitalocean.com/glossary/hypervisor/</a>) in several regions. In total, fourteen hypervisors crashed, the majority of which were in the FRA1 and AMS3 regions, the remaining being in LON1, SGP1, and NYC1. A routine kernel fix to improve platform stability was being deployed to a subset of hypervisors across the fleet, and that kernel fix had an unexpected conflict with a separate automated maintenance routine, causing those hypervisors to experience kernel panics and become unresponsive. This led to an interruption in service for customer Droplets, and other Droplet-based services until the affected hypervisors were rebooted and restored to a functional state.</p>

<h3 id="incident-details"><strong>Incident Details</strong></h3>

<ul>
<li><strong>Root Cause</strong>: A kernel fix being rolled out to some hypervisors through an incremental process conflicted with a periodic maintenance operation which was in progress on a subset of those hypervisors.</li>
<li><strong>Impact</strong>: The affected hypervisors crashed, causing Droplets (including other Droplet-based services) running on these hypervisors to become unresponsive. Customers were unable to reach them via networking, process events like power off/on, or see monitoring. </li>
<li><strong>Response</strong>: After gathering diagnostic information and determining the root cause, we rebooted the affected hypervisors in order to safely restore service. Manual remediation was done on hypervisors that received the kernel fix to ensure it was applied while the maintenance operation was not in progress.</li>
</ul>

<h3 id="timeline-of-events-utc"><strong>Timeline of Events (UTC)</strong></h3>

<p>July 24 22:55 - Rollout of the kernel fix begins. </p>

<p>July 24 23:10 - First hypervisor crash occurs and the Operations team begins investigating.</p>

<p>July 24 23:55 - Rollout of the kernel fix ends. </p>

<p>July 25 00:14 - Internal incident response begins, following further crash alerts firing. </p>

<p>July 25 00:35 - Diagnostic tests are run on impacted hypervisors to gather information.</p>

<p>July 25 00:47 - Kernel panic messages are observed on impacted hypervisors. Additional Engineering teams are paged for investigation.</p>

<p>July 25 01:42 - Operations team begins coordinated effort to reboot all impacted hypervisors to restore customer services.</p>

<p>July 25 01:50 - Root cause for the crashes is determined to be the conflict between the kernel fix and maintenance operation. </p>

<p>July 25 03:22 - Reboots of all impacted hypervisors complete, all services are restored to normal operation.</p>

<h3 id="remediation-actions"><strong>Remediation Actions</strong></h3>

<ul>
<li>The continued rollout of this specific kernel fix, as well as future rollouts of this type of fix, will not be done on hypervisors while the maintenance operation is in progress, to avoid any possible conflicts.</li>
<li>Further investigation will be conducted to understand how the kernel fix and the maintenance operation conflicted to cause a kernel crash to help avoid similar problems in the future.</li>
</ul>

            </p>

            <br>
            <small>
            
                Jul 25, 00:33 - Ongoing
            
            </small>
        </div>
        
        <div class="row">
            
                <div class="block">
                    <a href="http://127.0.0.1:8080/base/digitalocean/history/2"> ⮜ Previous </a>
                </div>
            
            
                <div class="block">
                    <a href="http://127.0.0.1:8080/base/digitalocean/history/0"> Next ⮞ </a>
                </div>
            
        </div>
    </div>
    
<footer></footer>

</body>

</html>
