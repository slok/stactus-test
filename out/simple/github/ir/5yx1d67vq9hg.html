
<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Github status</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.min.css">
    <link rel="stylesheet" href="http://127.0.0.1:8080/simple/github/static/main.css" />
    <script src="https://unpkg.com/@phosphor-icons/web"></script>
</head>


<body style="width:960px; margin:0 auto;">
    <header class="container">
        
<nav>
    <ul>
        <li>
            <h2>Github status </h2>
        </li>
    </ul>
    <ul>
        <li><a href="http://127.0.0.1:8080/simple/github/history/0">History</a></li>
        <li><a href="http://127.0.0.1:8080/simple/github/index">Status</a></li>
    </ul>
</nav>

    </header>
    <main class="container">
        <br />
        <h1 style="text-align: center;" class="text-major">Incident with Webhooks and Actions</h1>
        <br />
        
        <article class="incident-resolved">
            <i class="ph-bold ph-check"></i> <strong>Incident resolved in 3h52m43.971s</strong>
        </article>
        
        <br />
        
        <blockquote>
            <h4> Resolved </h4>
            <p>On July 5, 2024, between 16:31 UTC and 18:08 UTC, the Webhooks service was degraded, with customer impact of delays to all webhook delivery. On average, delivery delays were 24 minutes, with a maximum of 71 minutes. This was caused by a configuration change to the Webhooks service, which led to unauthenticated requests sent to the background job cluster. The configuration error was repaired and re-deploying the service solved the issue. However, this created a thundering herd effect which overloaded the background job queue cluster which put its API layer at max capacity, resulting in timeouts for other job clients, which presented as increased latency for API calls.<br /><br />Shortly after resolving the authentication misconfiguration, we had a separate issue in the background job processing service where health probes were failing, leading to reduced capacity in the background job API layer which magnified the effects of the thundering herd. From 18:21 UTC to 21:14 UTC, Actions runs on PRs experienced approximately 2 minutes delay and maximum of 12 minutes delay. A deployment of the background job processing service remediated the issue.<br /><br />To reduce our time to detection, we have streamlined our dashboards and added alerting for this specific runtime behavior. Additionally, we are working to reduce the blast radius of background job incidents through better workload isolation.<br /></p>

            <footer>
                <cite>Jul  5, 20:57</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are seeing recovery in Actions start times and are observing for any further impact.</p>

            <footer>
                <cite>Jul  5, 20:44</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are still seeing about 5% of Actions runs taking longer than 5 minutes to start. We are scaling and shifting resources to encourage recovery of the problem.</p>

            <footer>
                <cite>Jul  5, 20:32</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are still seeing about 5% of Actions runs taking longer than 5 minutes to start. We are evaluating mitigations to increase capacity to decrease latency.</p>

            <footer>
                <cite>Jul  5, 19:58</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are seeing about 5% of Actions runs not starting within 5 minutes. We are continuing investigation.</p>

            <footer>
                <cite>Jul  5, 19:19</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We have seen recovery of Actions run delays. Keeping the incident open to monitor for full recovery.</p>

            <footer>
                <cite>Jul  5, 18:40</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>Webhooks is operating normally.</p>

            <footer>
                <cite>Jul  5, 18:10</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are seeing delays in Actions runs due to the recovery with webhook deliveries. We expect this to resolve with the recovery of webhooks.</p>

            <footer>
                <cite>Jul  5, 18:09</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>Actions is experiencing degraded performance. We are continuing to investigate.</p>

            <footer>
                <cite>Jul  5, 18:07</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are seeing recovery as webhooks are being delivered again. We are burning down our queue of events. No events have been lost. New webhook deliveries will be delayed while this process recovers.</p>

            <footer>
                <cite>Jul  5, 17:57</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>Webhooks is experiencing degraded performance. We are continuing to investigate.</p>

            <footer>
                <cite>Jul  5, 17:55</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are reverting a configuration change that is suspected to contribute to the problem with webhook deliveries.</p>

            <footer>
                <cite>Jul  5, 17:42</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>Our telemetry shows that most webhooks are failing to be delivered. We are queueing all undelivered webhooks and are working to remediate the problem.</p>

            <footer>
                <cite>Jul  5, 17:20</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>Webhooks is experiencing degraded availability. We are continuing to investigate.</p>

            <footer>
                <cite>Jul  5, 17:17</cite>
            </footer>
        </blockquote>
        <hr />
        
        <blockquote>
            <h4> Investigating </h4>
            <p>We are investigating reports of degraded performance for Webhooks</p>

            <footer>
                <cite>Jul  5, 17:04</cite>
            </footer>
        </blockquote>
        <hr />
         
    </main>
    
<footer>
    <div class="container">
        Powered by <a href="https://github.com/slok/stactus">Stactus</a>.
    </div>
</footer>

</body>

</html>
